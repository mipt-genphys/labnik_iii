\begin{labsupplement}[Обработка экспериментальных данных:\\
    сводка результатов]

\labsection{Основные понятия}

\begin{description}[font=\mdseries\sffamily]
    \item[Выборочное среднее:] 
    \[
    \left<x\right> = \frac{1}{n} \sum\limits_{i=1}^{n} x_i,
    \]
    где $\{x_i\}$ --- результаты $n$ однотипных измерений одной и той же величины.
    \item[Выборочная дисперсия:]
    \[
    s^2_{n} = \left<(x-\left<x\right>)^2\right> = 
            \frac{1}{n} \sum\limits_{i=1}^n (x_i - \left<x\right>)^2,
    \]
    \[
    s^2_n = \left<x^2\right> - \left<x\right>^2.
    \]
    \item[Несмещенная оценка дисперсии:]
    \[
    s^2_{n-1} = 
    \frac{n}{n-1} s_n^2 = 
    \frac{1}{n-1} \sum\limits_{i=1}^{n} (x_i - \left<x\right>)^2.
    \]
    Несмещенную оценку необходимо применять при малых $n$ ($n\lesssim 10$).
    \item[Погрешность среднего значения:]
    \[
    \sigma_{\left<x\right>} = \frac{\sigma_x}{\sqrt{n}},
    \]
    где $n$ --- число измерений, $\sigma_x$ --- оценка среднеквадратичного
    отклонения результатов отдельных измерений.
    \item[Сложение случайной и систематической погрешностей:]
    \[
    \sigma_{полн}^2 \le \sqrt{\Delta_{сист}^2 + \sigma^2_{случ}},
    \]
    где $\sigma_{сист}$ --- полученная из опыта оценка случайной 
    (среднеквадратичной) погрешности, 
    $\Delta_{сист}$ --- оценка максимальной величины систематической погрешности.
\end{description}

\newpage

\labsection{Случайные распределения}

\labsubsection{Нормальное распределение}
Величина, являющаяся результатом суммы большого числа \emph{независимых}
случайных слагаемых, имеющих конечные среднее и дисперсию, подчиняется
\emph{нормальному распределению} (\emph{распределению Гаусса}).

Плотность вероятности нормального распределения:
\[
w_{\mathcal N}(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(x-\overline{x})^2}{2\sigma^2}\right).
\]
Параметры распределения: $\overline{x}$ --- среднее значение (\emph{математическое ожидание}),
$\sigma^2$ --- дисперсия.

Вероятность нормально распределённой величине попасть в интервал
$x\in (\overline{x}-\delta; \overline{x}+\delta)$:
\[
P({|x-\overline{x}|<\delta})  = 
\int\limits_{-\delta}^{\delta} \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{2\sigma^2}} = 
\mathop{\mathrm{erf}}\left(\frac{\delta}{\sqrt{2}\sigma}\right),
\]
где $\mathrm{erf}(x)$ --- \emph{интеграл ошибок}:
\[
\mathop{\mathrm{erf}} (x) \equiv \frac{2}{\sqrt{\pi}} \int\limits_0^x e^{-x^2} dx.
\]

Вероятности попасть в интервал $x\in(\overline{x}-\delta,\,\overline{x}+\delta)$ 
представлены в табл.~1.
Графики зависимостей нормального распределения и интеграла ошибок
приведены на рис.~??.
\begin{table}[h!]\centering
    \small
    \begin{tabular}{cc}
        $x-\overline{x}$ & Вероятность \\ \hline
        $\pm \sigma\phantom{0}$ & 0,68 \\
        $\pm 2\sigma$ & 0,95 \\
        $\pm 3\sigma$ & 0,9973 \\
        $\pm 4\sigma$ & $1-6,3\cdot 10^{-5}$ \\
        $\pm 5\sigma$ & $1-5,7\cdot 10^{-7}$ \\
    \end{tabular}
\end{table}    

\begin{figure}\centering
    \pic{8.5cm}{gauss}
    \caption{Плотность нормального распределения}
\end{figure}

\begin{figure}\centering\small
    \pic{\textwidth}{erf1}
    \caption{Вероятность отклонения  от среднего при нормальном распределении: 
    слева вероятность попасть в интервал $x\in(\overline{x}-\delta,\overline{x}+\delta)$, справа ---
     вероятность выйти за его пределы (в логарифмическом масштабе)}
\end{figure}

\labsubsection{Распределение Пуассона}
Равномерные во времени дискретные \emph{независимые} случайные события 
подчиняются \emph{распределению Пуассона}:
\[
w(n) = \frac{\nu^n}{n!}e^{-\nu}
\]
--- вероятность обнаружить~$n$ событий в течение некоторого
фиксированного времени наблюдений. Здесь $\nu$~--- параметр распределения,
имеющий смысл среднего числа событий:
\[
\overline{n} = \sum\limits_{n=0}^{\infty} n w(n) = \nu.
\]
Дисперсия распределения Пуассона:
\[
\sigma^2 = \sum\limits_{n=0}^{\infty} (n-\overline{n})^2 w(n) = \overline{n}.
\]

В пределе большого числа событий ($\overline{n} \gg 1$) распределение Пуассона
стремится к нормальному со средним $\overline{n}$ и 
дисперсией $\sigma^2 = \overline{n}$.

\labsection{Вычисление косвенных погрешностей}
Пусть величина $u$ вычисляется косвенно по известной функции $u=f(x,\,y,\ldots)$, 
где ${x,\,y,\,\ldots}$ --- \emph{независимые} случайные величины,
полученные в результате прямых измерений. 

\begin{description}[font=\mdseries\sffamily]
\item[Общая формула вычисления погрешности:]
\[
\sigma_u = \sqrt{ (f'_x \sigma_x)^2 + (f'_y \sigma_y)^2 + \ldots},
\]
где $f'_x \equiv \frac{\partial f}{\partial x}$, 
$f'_y \equiv \frac{\partial f}{\partial y}$, \ldots --- частные производные функции
$f$, вычисленные в точке $(x,\,y,\,\ldots)$, а
$\sigma_x$, $\sigma_y$, \ldots --- среднеквадратичные погрешности измерений.

Формула применима для независимых измерений и при малых относительных 
отклонениях исследуемой величины ($\sigma_u / u \ll 1$).
Ниже приведены её частные случаи.


\item[Погрешность суммы/разности:]
\[
u = x\pm y\qquad\to \qquad \sigma_u = \sqrt{\sigma_x^2 + \sigma_y^2}.
\]

\item[Погрешность линейной комбинации:]
\[
u = \sum a_i x_i \qquad \to \qquad \sigma_u = \sqrt{\sum (a_i \sigma_{xi})^2}.
\]

\item[Погрешность произведения:]
\[
u = x y \qquad\to \qquad \frac{\sigma_u}{u} = 
\sqrt{\left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2}.
\]

\item[Погрешность частного:]
\[
u = \frac{x}{y} \qquad \to \qquad \frac{\sigma_u}{u} = 
\sqrt{\left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2}.
\]

\item[Погрешность степенной функции:]
\[
u = x^{\alpha} y^{\beta} \qquad \to \qquad \frac{\sigma_u}{u} = 
\sqrt{\left(\alpha \frac{\sigma_x}{x}\right)^2 + \left(\beta \frac{\sigma_y}{y}\right)^2}.
\]
\item[Погрешность логарифма:]
\[
u = \ln x \qquad \to \qquad \sigma_u = \frac{\sigma_x}{x}
\]
\end{description}


\labsection{Оценка параметров зависимостей}
    
Пусть теоретическая модель предсказывает функциональную зависимость величин~$y$ и~$x$ 
вида 
\[
y=f(x; \vec{\theta}), 
\]
где $\vec{\theta}=(\theta_1,\,\theta_2,\,\ldots,\,\theta_p)$~--- 
некоторый набор из $p$ параметров модели.
Задача об оценке параметров заключается в нахождении значений 
$(\theta_1,\,\theta_2\,\ldots)$, при которых данная теоретическая зависимость 
наилучшим образом <<ложится>> на имеющийся 
набор $n$ экспериментальных точек $(x_i,\,y_i)$, $i=1\ldots n$, 
минимизируя некоторым образом набор отклонений от теории
\[
\Delta y_i \equiv y_i - f(x_i; \vec{\theta}) \;\to \; \min.
\]

    
\labsubsection{Метод наименьших квадратов}    
Метод наименьших квадратов (МНК) заключается в поиске набора параметров $\vec{\theta}$,
минимизирующего сумму квадратов отклонений по $y$:
\[
S(\vec{\theta}) = \sum\limits_{i=1}^n \Delta y_i^2 \to \min.
\]
Применение метода обосновано при условии, что
\begin{itemize}[itemsep=0pt]
    \item измерения \emph{независимы},
    \item все погрешности в основном \emph{случайны} и распределены \emph{нормально},
    \item погрешность по $x$ мала: $\frac{\sigma_{x}}{x} \ll \frac{\sigma_y}{y}$,
    \item все погрешности по $y$ \emph{одинаковы}: $\sigma_{yi} = \sigma_y = \const$.
\end{itemize}

\begin{description}[font=\mdseries\sffamily]
    \item[Аппроксимация по МНК для линейной зависимости $y=kx+b$:]
    \begin{equation}\eqmark{mnk_kb}
         k = \frac{D_{xy}}{D_{xx}},\qquad  b = \left<y\right> - k \left<x\right>,
    \end{equation}
    где обозначено
    \[
        D_{xy} = \left<x y\right> - \left<x\right> \left<y\right>,
        \quad D_{xx} = \left<x^2\right> - \left<x\right>^2.
    \]
    \item[Погрешность апроксимации $y=kx+b$:]
    \[
    \sigma_k = \sqrt{\frac{1}{n-2}\left(\frac{D_{yy}}{D_{xx}}-k^2\right)},\qquad
    \sigma_b = \sigma_{k} \sqrt{\left<x^2\right>}.
    \]
    \item[Аппроксимация по МНК для линейной зависимости $y=kx$:]
    \[
    k = \frac{\left<xy\right>}{\left<x^2\right>}.
    \]
    \item[Погрешность апроксимации $y=kx$:]
    \[
    \sigma_k = 
    \sqrt{\frac{1}{n-1}\left(\frac{\left<y^2\right>}{\left<x^2\right>}-k^2\right)}.
    \]
\end{description}
Аппроксимация \emph{нелинейной} зависимости часто может быть произведена по формулам 
линейной аппроксимации с помощью замены переменных. Например,
\[
y = kx^2 \quad \to \quad y=kz,\; z = x^2.
\]
В общем случае минимизация суммы $S(\vec{\theta})$ может быть выполнена
численными методами.

\labsubsection{Метод хи-квадрат}
Суммой \emph{хи-квадрат} называют сумму квадратов отклонений $\Delta y_i$ от
некоторой теоретической зависимости $y=f(x;\vec{\theta})$, 
нормированных на соответствующие погрешности $\sigma_{yi}$:
\begin{equation}\eqmark{chi2}
\chi^2_n \equiv \sum\limits_{i=1}^{n} \left(\frac{\Delta y_i}{\sigma_{yi}}\right)^2.
\end{equation}
Метод \emph{минимума хи-квадрат} (\emph{метод Пирсона}) заключается
в поиске набора параметров $\vec{\theta}$, минимизирующих сумму \eqref{chi2}:
\[
\chi^2 (\vec{\theta}) \to \min.
\]
По сравнению с МНК данный метод позволяет учесть, что погрешности 
измерений различны: $\sigma_{yi} \ne \const$. При этом величины
по-прежнему должны быть \emph{независимыми}, \emph{нормально распределенными},
а погрешность по $x$ мала.

\begin{description}[font=\mdseries\sffamily]
    \item[Аппроксимация по хи-квадрат для линейной зависимости $y=kx+b$:]
        формулы аналогичны \eqref{mnk_kb}:
        \[
k = \frac{\left<x y\right>' - \left<x\right>' \left<y\right>'}{\left<x^2\right>' - \left<x\right>'^2},\qquad  
b = \left<y\right>' - k \left<x\right>',
        \]
где под знаком $\left<\ldots\right>'$ 
        необходимо понимать \emph{взвешенное среднее} с весами~$1/\sigma_{yi}^2$:
        \[
        \left<x\right>' = \frac{\sum\limits w_i x_i}{\sum w_i},\qquad 
        w_i = \frac{1}{\sigma_{yi}^2}.
        \]
    \item[Аппроксимация произвольной зависимости]
   проводится численно. Метод реализован во многих пакетах
   обработки данных.
   
     \begin{figure}\small
         \pic{\textwidth}{chisquare}
         \caption{Проверка качества аппроксимации с помощью метода хи-квадрат}
        \end{figure}
        
        
    \item[Погрешность параметра апроксимации $\sigma_{\theta}$]
    может быть определена из того факта, что функция $\chi^2(\theta)$ 
    вблизи максимума имеет вид параболы:
    \[
    \chi^2 \approx \frac{(\theta - \theta_0)^2}{\sigma_{\theta}^2} + \const,
    \]
    где $\theta_0$ --- параметр, минимизирующий $\chi^2$.
    Таким образом, отклонение значения $\chi^2$ на единицу соответствует
    вариации параметра $\theta$ на величину его среднеквадратичной погрешности 
    $\sigma_{\theta}$:
    \[
    \chi^2(\theta_0 \pm \sigma_{\theta}) - \chi^2(\theta_0) = 1.
    \]
    \item[Проверка качества аппроксимации:] метод хи-квадрат позволяет
    проверить, насколько экспериментальные данные описываются
    моделью $y=f(x;\vec{\theta})$. 
    При фиксированных параметрах $\vec{\theta}$ 
    сумма хи-квадрат является случайной величиной с математическим
    ожиданием (см. [2, 3])
    \begin{equation*}\eqmark{chi2-np}
    \overline{\chi^2_n} = n - p,
    \end{equation*}
    где $p$ --- число параметров модели ($p=2$ для $y=kx+b$),
    $n-p$ --- число степеней свободы при решении задачи о максимизации
    $\chi^2$.
    
    Существенное отклонение от единицы величины 
    \[
     \alpha = \frac{\chi^2}{n-p}
    \] 
 свидетельствует о плохом качестве аппроксимации 
 или о неправильной оценке погрешностей.
    Большие значения ($\alpha >1,5 \div 2$) --- признак либо \emph{плохого совпадения теоретической
    модели и эксперимента}, либо \emph{заниженной} оценки погрешностей.
    Малые значения ($\alpha <0,5$) свидетельствуют, как правило, о
    \emph{завышенных} величинах погрешностей.
    
    Если измеряемые величины распределены нормально, распределение суммы
    $\chi^2$ называют \emph{распределением хи-квадрат} с $n-p$ степенями
    свободы. Соответствующая функция встроена во все основные 
    статистические пакеты. На рис.~?? представлен график, по которому 
    можно оценить качество аппроксимации.
    Например, если полученное на опыте значение $\chi^2/(n-p)$ лежит выше 
    уровня $P=5\%$ на графике, то лишь с вероятностью 5\%  отклонение от 
    теории можно объяснить случайными факторами.
    
  
    
\end{description}

\begin{lab:literature}
    \item \textit{Попов П.В., Нозик А.А.} Обработка результатов
    учебного эксперимента.~--- М.\,: МФТИ, 2019.

%    \item \textit{Тейлор Дж}. Введение в теорию ошибок.
%    
%    \item \textit{Сквайрс Дж.} Практическая физика.
%    
%    \item \textit{Зайдель А.Н.} Погрешности измерений физических величин.
    
    \item \textit{Худсон Д.} Статистика для физиков.
    
    \item \textit{Идье В.}, \textit{Драйард Д.}, \textit{Джеймс Ф}., \textit{Рус М.}, \textit{Садуле Б.} 
    Статистические методы в экспериментальной физике.
\end{lab:literature}

\end{labsupplement}