\begin{booksupplement}[Обработка экспериментальных данных]

\begin{center}
\normalsize\bfseries\sffamily (сводка результатов)
\end{center}

\labsection{Основные понятия}

\begin{description}[font=\mdseries\sffamily]
    \item[Выборочное среднее:] 
    \[
    \left<x\right> = \frac{1}{n} \sum\limits_{i=1}^{n} x_i,
    \]
    где $\{x_i\}$ --- результаты $n$ однотипных измерений одной и той же величины.
    \item[Выборочная дисперсия:]
    \[
    s^2_{n} = \left<(x-\left<x\right>)^2\right> = 
            \frac{1}{n} \sum\limits_{i=1}^n (x_i - \left<x\right>)^2,
    \]
    \[
    s^2_n = \left<x^2\right> - \left<x\right>^2.
    \]
    \item[Несмещенная оценка дисперсии:]
    \[
    s^2_{n-1} = 
    \frac{n}{n-1} s_n^2 = 
    \frac{1}{n-1} \sum\limits_{i=1}^{n} (x_i - \left<x\right>)^2.
    \]
    Несмещенную оценку необходимо применять при малых $n$ ($n\lesssim 10$).
    \item[Погрешность среднего значения:]
    \[
    \sigma_{\left<x\right>} = \frac{\sigma_x}{\sqrt{n}},
    \]
    где $n$ --- число измерений, $\sigma_x$ --- оценка среднеквадратичного
    отклонения результатов отдельных измерений.
    \item[Сложение случайной и систематической погрешностей:]
    \[
    \Delta_{полн} \le \sqrt{\Delta_{сист}^2 + \sigma^2_{случ}},
    \]
    где $\sigma_{случ}$ --- полученная из опыта оценка случайной 
    (среднеквадратичной) погрешности, 
    $\Delta_{сист}$ --- оценка максимальной величины систематической погрешности.
\end{description}

\newpage

\labsection{Случайные распределения}

\labsubsection{Нормальное распределение}
Согласно центральной предельной теореме, величина, являющаяся результатом суммы большого числа \emph{независимых}
случайных слагаемых, имеющих конечные среднее и дисперсию, подчиняется
\emph{нормальному распределению} (\emph{распределению Гаусса}).

\emph{Плотность вероятности} нормального распределения:
\[
w_{\mathcal N}(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(x-\overline{x})^2}{2\sigma^2}\right).
\]
Параметры распределения: $\overline{x}$ --- среднее значение (\emph{математическое ожидание}),
$\sigma^2$ --- дисперсия. 
Графики плотности нормального распределения представлен на рис.~\figref{gauss}.
Вероятность попасть в интервал
$x\in (\overline{x}-\delta; \overline{x}+\delta)$:
\[
P({|x-\overline{x}|<\delta})  = 
\int\limits_{-\delta}^{\delta} w_{\mathcal{N}}\,dx = 
\mathop{\mathrm{erf}}\left(\frac{\delta}{\sqrt{2}\sigma}\right),
\]
где $\mathrm{erf}(x)$ --- \emph{интеграл ошибок}:
\[
\mathop{\mathrm{erf}} (x) \equiv \frac{2}{\sqrt{\pi}} \int\limits_0^x e^{-x^2} dx.
\]
Графики соответствующей функции в обычном и логарифмическом масштабе 
приведены на рис.~\figref{erf}.

%Вероятности попасть в интервал $x\in(\overline{x}-\delta,\,\overline{x}+\delta)$ 
%представлены в табл.~1.
%Графики зависимостей нормального распределения и интеграла ошибок
%приведены на рис.~\figref{gauss} и рис.~\figref{erf} соответственно.

\etp{-2}

\begin{figure}[h!]\centering
    \pic{8.8cm}{gauss}\small
    \caption{Плотность нормального распределения}
    \figmark{gauss}
\end{figure}

\begin{figure}[h!]\centering\small
    \pic{0.9\textwidth}{erf1}\small
    \caption{Вероятность отклонения  от среднего $\delta = x - \bar{x}$ 
        при нормальном распределении: 
        слева --- вероятность попасть в интервал $x\in(\overline{x}-\delta,\overline{x}+\delta)$, справа ---
     вероятность выйти за его пределы (в логарифмическом масштабе)}
    \figmark{erf}
\end{figure}

При записи результата измерений в виде \emph{центрального интервала}
\[
x = \overline{x} \pm \Delta x
\]
по умолчанию (если не указано иное) подразумевается, что физическая величина 
имеет нормальное распределение со средним значением $\overline{x}$ и 
среднеквадратичным отклонением $\sigma=\Delta x$.
Вероятностное содержания интервала 
$(\overline{x} -\sigma, \overline{x} + \sigma)$
для нормального распределения составляет 68,3\%. 
%Таким образом, запись вида $R = 2.0 \pm 0.1$ как правило означает, 
%что погрешности для $R$ распределены по нормальному закону c $\sigma = 0.1$ 
%и примерно в двух третях случаев истинное значение лежит в интервале (1.9, 2.1). 
В особо точных измерениях для представления 
окончательных могут использоваться $2\sigma$, $3\sigma$ 
или даже $5\sigma$ интервалы.
Соответствующее им вероятностное содержание представлено в таблице:\par
%\begin{table}[h!]\centering
\hfil\begingroup
\small
    \begin{tabular}{cc}
        $x-\overline{x}$ & Вероятность \\ \hline
        $\pm \sigma\phantom{0}$ & 0,683 \\
        $\pm 2\sigma$ & 0,954 \\
        $\pm 3\sigma$ & 0,9973 \\
        $\pm 4\sigma$ & $1-6,3\cdot 10^{-5}$ \\
        $\pm 5\sigma$ & $1-5,7\cdot 10^{-7}$ \\
    \end{tabular}
\endgroup    
\smallskip
%\end{table}    

\labsubsection{Распределение Пуассона}
Равномерные во времени дискретные \emph{независимые} друг от друга 
случайные события подчиняются \emph{распределению Пуассона}:
\[
w(n) = \frac{\nu^n}{n!}e^{-\nu}
\]
--- вероятность обнаружить~$n$ событий в течение фиксированного времени наблюдений. Здесь $\nu$~--- параметр распределения,
имеющий смысл среднего числа событий:
\[
\overline{n} = \sum\limits_{n=0}^{\infty} n w(n) = \nu.
\]
Дисперсия распределения Пуассона:
\[
\sigma^2 = \sum\limits_{n=0}^{\infty} (n-\overline{n})^2 w(n) = \overline{n}.
\]

В пределе большого числа событий ($\overline{n} \gg 1$) распределение Пуассона
стремится к нормальному со средним $\overline{n}$ и 
дисперсией $\sigma^2 = \overline{n}$.

\labsection{Вычисление косвенных погрешностей}
Пусть величина $u$ вычисляется косвенно по известной функции $u=f(x,\,y,\ldots)$, 
где ${x,\,y,\,\ldots}$ --- \emph{независимые} случайные величины,
полученные в результате измерений. 

\begin{description}[font=\mdseries\sffamily]
\item[Общая формула вычисления погрешности:]
\[
\sigma_u = \sqrt{ (f'_x \sigma_x)^2 + (f'_y \sigma_y)^2 + \ldots},
\]
где $f'_x \equiv \frac{\partial f}{\partial x}$, 
$f'_y \equiv \frac{\partial f}{\partial y}$, \ldots --- частные производные функции
$f$, вычисленные в точке $(x,\,y,\,\ldots)$, а
$\sigma_x$, $\sigma_y$, \ldots --- среднеквадратичные погрешности измерений.

Формула применима для независимых измерений и при малых относительных 
отклонениях исследуемой величины ($\sigma_u / u \ll 1$).
Ниже приведены её частные случаи.


\item[Погрешность суммы/разности:]
\[
u = x\pm y\qquad\to \qquad \sigma_u = \sqrt{\sigma_x^2 + \sigma_y^2}.
\]

\item[Погрешность линейной комбинации:]
\[
u = \sum a_i x_i \qquad \to \qquad \sigma_u = \sqrt{\sum (a_i \sigma_{xi})^2}.
\]

\item[Погрешность произведения:]
\[
u = x y \qquad\to \qquad \frac{\sigma_u}{u} = 
\sqrt{\left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2}.
\]

\item[Погрешность частного:]
\[
u = \frac{x}{y} \qquad \to \qquad \frac{\sigma_u}{u} = 
\sqrt{\left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2}.
\]

\item[Погрешность степенной функции:]
\[
u = x^{\alpha} y^{\beta} \qquad \to \qquad \frac{\sigma_u}{u} = 
\sqrt{\left(\alpha \frac{\sigma_x}{x}\right)^2 + \left(\beta \frac{\sigma_y}{y}\right)^2}.
\]
\item[Погрешность логарифма:]
\[
u = \ln x \qquad \to \qquad \sigma_u = \frac{\sigma_x}{x}
\]
\end{description}


\labsection{Оценка параметров зависимостей}
    
Пусть теоретическая модель предсказывает функциональную зависимость величин~$y$ и~$x$ 
вида 
\[
y=f(x; \vec{\theta}), 
\]
где $\vec{\theta}=(\theta_1,\,\theta_2,\,\ldots,\,\theta_p)$~--- 
некоторый набор из $p$ параметров модели.
Задача об оценке параметров заключается в нахождении значений 
$(\theta_1,\,\theta_2\,\ldots)$, при которых данная теоретическая зависимость 
наилучшим образом <<ложится>> на имеющийся 
набор $n$ экспериментальных точек $(x_i,\,y_i)$, $i=1\ldots n$, 
минимизируя некоторым образом набор отклонений от теории
\[
\Delta y_i \equiv y_i - f(x_i; \vec{\theta}) \;\to \; \min.
\]

    
\labsubsection{Метод наименьших квадратов}    
Метод наименьших квадратов (МНК) заключается в поиске набора параметров $\vec{\theta}$,
минимизирующего сумму квадратов отклонений по $y$:
\[
S(\vec{\theta}) = \sum\limits_{i=1}^n \Delta y_i^2 \to \min.
\]
Применение метода обосновано при условии, что
\begin{itemize}[itemsep=0pt]
    \item измерения \emph{независимы},
    \item все погрешности в основном \emph{случайны} и распределены \emph{нормально},
    \item погрешность по $x$ мала: $\frac{\sigma_{x}}{x} \ll \frac{\sigma_y}{y}$,
    \item все погрешности по $y$ \emph{одинаковы}: $\sigma_{yi} = \sigma_y = \const$.
\end{itemize}

\begin{description}[font=\mdseries\sffamily]
    \item[Аппроксимация по МНК для линейной зависимости $y=kx+b$:]
    \begin{equation}\eqmark{mnk_kb}
         k = \frac{D_{xy}}{D_{xx}},\qquad  b = \left<y\right> - k \left<x\right>,
    \end{equation}
    где обозначено
    \[
        D_{xy} = \left<x y\right> - \left<x\right> \left<y\right>,
        \quad D_{xx} = \left<x^2\right> - \left<x\right>^2.
    \]
    \item[Погрешность аппроксимации $y=kx+b$:]
    \[
    \sigma_k = \sqrt{\frac{1}{n-2}\left(\frac{D_{yy}}{D_{xx}}-k^2\right)},\qquad
    \sigma_b = \sigma_{k} \sqrt{\left<x^2\right>}.
    \]
    \item[Аппроксимация по МНК для линейной зависимости $y=kx$:]
    \[
    k = \frac{\left<xy\right>}{\left<x^2\right>}.
    \]
    \item[Погрешность аппроксимации $y=kx$:]
    \[
    \sigma_k = 
    \sqrt{\frac{1}{n-1}\left(\frac{\left<y^2\right>}{\left<x^2\right>}-k^2\right)}.
    \]
\end{description}
Аппроксимация \emph{нелинейной} зависимости часто может быть произведена по формулам 
линейной аппроксимации с помощью замены переменных. Например,
\[
y = kx^2 \quad \to \quad y=kz,\; z = x^2.
\]
В общем случае минимизация суммы $S(\vec{\theta})$ может быть выполнена
численными методами.

\labsubsection{Метод хи-квадрат}
Суммой \emph{хи-квадрат} называют сумму квадратов отклонений $\Delta y_i$ от
некоторой теоретической зависимости $y=f(x;\vec{\theta})$, 
нормированных на соответствующие погрешности $\sigma_{yi}$:
\begin{equation}\eqmark{chi2}
\chi^2_n \equiv \sum\limits_{i=1}^{n} \left(\frac{\Delta y_i}{\sigma_{yi}}\right)^2.
\end{equation}
Метод \emph{минимума хи-квадрат} (\emph{метод Пирсона}) заключается
в поиске набора параметров $\vec{\theta}$, минимизирующих сумму \eqref{chi2}:
\[
\chi^2 (\vec{\theta}) \to \min.
\]
По сравнению с МНК данный метод позволяет учесть, что погрешности 
измерений различны: $\sigma_{yi} \ne \const$. При этом величины
по-прежнему должны быть \emph{независимыми}, \emph{нормально распределенными},
а погрешность по $x$ мала.

\begin{description}[font=\mdseries\sffamily]
    \item[Аппроксимация по хи-квадрат для линейной зависимости $y=kx+b$:]
        формулы аналогичны \eqref{mnk_kb}:
        \[
k = \frac{\left<x y\right>' - \left<x\right>' \left<y\right>'}{\left<x^2\right>' - \left<x\right>'^2},\qquad  
b = \left<y\right>' - k \left<x\right>',
        \]
где под знаком $\left<\ldots\right>'$ 
        необходимо понимать \emph{взвешенное среднее} с весами~$1/\sigma_{yi}^2$:
        \[
        \left<x\right>' = \frac{\sum\limits w_i x_i}{\sum w_i},\qquad 
        w_i = \frac{1}{\sigma_{yi}^2}.
        \]
    \item[Аппроксимация произвольной зависимости]
   проводится численно. Метод реализован во многих пакетах обработки данных.
   В~языке \textit{Python} можно использовать функцию
   \texttt{scipy.optimize.curve\_fit} модуля \texttt{optimize} 
   пакета \texttt{scipy}.
%   https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html.

   
   \item[Погрешность параметра аппроксимации $\sigma_{\theta}$]
    может быть определена численно из того факта, что функция $\chi^2(\theta)$ 
    вблизи максимума как правило имеет вид параболы:
    \[
    \chi^2 \approx \frac{(\theta - \theta_0)^2}{\sigma_{\theta}^2} + \const,
    \]
    где $\theta_0$ --- параметр, минимизирующий $\chi^2$.
    Таким образом, отклонение значения $\chi^2$ на единицу соответствует
    вариации параметра $\theta$ на величину его среднеквадратичной погрешности 
    $\sigma_{\theta}$:
    \[
    \chi^2(\theta_0 \pm \sigma_{\theta}) - \chi^2(\theta_0) = 1.
    \]
\end{description}

\begin{description}[font=\mdseries\sffamily]       
    \item[Проверка качества аппроксимации:] метод хи-квадрат позволяет
    проверить, насколько экспериментальные данные описываются
    моделью $y=f(x;\vec{\theta})$. 
    При фиксированных параметрах $\vec{\theta}$ 
    сумма хи-квадрат с $n-p$ степенями свободы,
    где $p$ --- число параметров модели ($p=2$ для $y=kx+b$),
    является случайной величиной с математическим
    ожиданием (см. [3, 4])
    \begin{equation*}\eqmark{chi2-np}
    \overline{\chi^2_{n-p}} = n - p.
    \end{equation*}
    
    Существенное отклонение от единицы величины 
    \[
     \alpha = \frac{\chi^2}{n-p}
    \] 
    свидетельствует о плохом качестве аппроксимации или о неправильной оценке погрешностей.
    Большие значения ($\alpha >1,5 \div 2$) --- признак либо \emph{плохого совпадения теоретической
    модели и эксперимента}, либо \emph{заниженной} оценки погрешностей.
    Малые значения ($\alpha <0,5$) свидетельствуют, как правило, о
    \emph{завышенных} величинах погрешностей.
    
    Если измеряемые величины распределены \emph{нормально}, распределение суммы
    $\chi^2$ называют <<\emph{распределением хи-квадрат}>> с $n-p$ степенями
    свободы. Соответствующая функция встроена во все основные 
    статистические пакеты. В языке \emph{Python} распределение 
    реализовано в виде модуля \texttt{stats.distributions.chi2}
    пакета \texttt{scipy}.
    
    На рис.~\figref{chisquare} представлен график, 
    по которому     можно оценить качество аппроксимации. Линии 
    $P=\const$ соответствуют вероятности того, что 
    полученное на опыте значение $\chi^2/(n-p)$ с вероятностью $P$ 
    возникло в результате воздействия случайных факторов.

    \begin{figure}[h!]\small\centering
        \pic{0.95\textwidth}{chisquare}
        \caption{Проверка качества аппроксимации с помощью метода хи-квадрат
            (для нормального распределения ошибок)}
        \figmark{chisquare}
    \end{figure}    
   
\end{description}


\newpage

\begin{lab:literature}
    \item \textit{Попов П.В., Нозик А.А.} Обработка результатов
    учебного эксперимента.~--- М.\,: МФТИ, 2019.

    \item \textit{Тейлор Дж.} Введение в теорию ошибок.~---
    М.\,: Мир, 1985.
%    
%    \item \textit{Сквайрс Дж.} Практическая физика.
%    
%    \item \textit{Зайдель А.Н.} Погрешности измерений физических величин.
    
    \item \textit{Худсон Д.} Статистика для физиков.~---
    М.\,: Мир, 1970.
    
    \item \textit{Идье В.}, \textit{Драйард Д.}, \textit{Джеймс Ф}., \textit{Рус М.}, \textit{Садуле Б.} 
    Статистические методы в экспериментальной физике.~---
    М.\,: Атомиздат, 1976. 
\end{lab:literature}

\end{booksupplement}
